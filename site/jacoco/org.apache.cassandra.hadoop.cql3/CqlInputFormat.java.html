<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>CqlInputFormat.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">JaCoCo Ant Example</a> &gt; <a href="index.source.html" class="el_package">org.apache.cassandra.hadoop.cql3</a> &gt; <span class="el_source">CqlInputFormat.java</span></div><h1>CqlInputFormat.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.hadoop.cql3;

import java.io.IOException;
import java.util.*;
import java.util.concurrent.*;

import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.Host;
import com.datastax.driver.core.Metadata;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.TokenRange;

import org.apache.cassandra.config.SchemaConstants;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapred.InputSplit;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.RecordReader;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.TaskAttemptID;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.cassandra.db.SystemKeyspace;
import org.apache.cassandra.dht.*;
import org.apache.cassandra.thrift.KeyRange;
import org.apache.cassandra.hadoop.*;

import static java.util.stream.Collectors.toMap;

/**
 * Hadoop InputFormat allowing map/reduce against Cassandra rows within one ColumnFamily.
 *
 * At minimum, you need to set the KS and CF in your Hadoop job Configuration.  
 * The ConfigHelper class is provided to make this
 * simple:
 *   ConfigHelper.setInputColumnFamily
 *
 * You can also configure the number of rows per InputSplit with
 *   1: ConfigHelper.setInputSplitSize. The default split size is 64k rows.
 *   or
 *   2: ConfigHelper.setInputSplitSizeInMb. InputSplit size in MB with new, more precise method
 *   If no value is provided for InputSplitSizeInMb, we default to using InputSplitSize.
 *
 *   CQLConfigHelper.setInputCQLPageRowSize. The default page row size is 1000. You
 *   should set it to &quot;as big as possible, but no bigger.&quot; It set the LIMIT for the CQL 
 *   query, so you need set it big enough to minimize the network overhead, and also
 *   not too big to avoid out of memory issue.
 *   
 *   other native protocol connection parameters in CqlConfigHelper
 */
<span class="nc bnc" id="L71" title="All 2 branches missed.">public class CqlInputFormat extends org.apache.hadoop.mapreduce.InputFormat&lt;Long, Row&gt; implements org.apache.hadoop.mapred.InputFormat&lt;Long, Row&gt;</span>
{
    public static final String MAPRED_TASK_ID = &quot;mapred.task.id&quot;;
<span class="nc" id="L74">    private static final Logger logger = LoggerFactory.getLogger(CqlInputFormat.class);</span>
    private String keyspace;
    private String cfName;
    private IPartitioner partitioner;

    public RecordReader&lt;Long, Row&gt; getRecordReader(InputSplit split, JobConf jobConf, final Reporter reporter)
            throws IOException
    {
<span class="nc" id="L82">        TaskAttemptContext tac = HadoopCompat.newMapContext(</span>
                jobConf,
<span class="nc" id="L84">                TaskAttemptID.forName(jobConf.get(MAPRED_TASK_ID)),</span>
                null,
                null,
                null,
                new ReporterWrapper(reporter),
                null);


<span class="nc" id="L92">        CqlRecordReader recordReader = new CqlRecordReader();</span>
<span class="nc" id="L93">        recordReader.initialize((org.apache.hadoop.mapreduce.InputSplit)split, tac);</span>
<span class="nc" id="L94">        return recordReader;</span>
    }

    @Override
    public org.apache.hadoop.mapreduce.RecordReader&lt;Long, Row&gt; createRecordReader(
            org.apache.hadoop.mapreduce.InputSplit arg0, TaskAttemptContext arg1) throws IOException,
            InterruptedException
    {
<span class="nc" id="L102">        return new CqlRecordReader();</span>
    }

    protected void validateConfiguration(Configuration conf)
    {
<span class="nc bnc" id="L107" title="All 4 branches missed.">        if (ConfigHelper.getInputKeyspace(conf) == null || ConfigHelper.getInputColumnFamily(conf) == null)</span>
        {
<span class="nc" id="L109">            throw new UnsupportedOperationException(&quot;you must set the keyspace and table with setInputColumnFamily()&quot;);</span>
        }
<span class="nc bnc" id="L111" title="All 2 branches missed.">        if (ConfigHelper.getInputInitialAddress(conf) == null)</span>
<span class="nc" id="L112">            throw new UnsupportedOperationException(&quot;You must set the initial output address to a Cassandra node with setInputInitialAddress&quot;);</span>
<span class="nc bnc" id="L113" title="All 2 branches missed.">        if (ConfigHelper.getInputPartitioner(conf) == null)</span>
<span class="nc" id="L114">            throw new UnsupportedOperationException(&quot;You must set the Cassandra partitioner class with setInputPartitioner&quot;);</span>
<span class="nc" id="L115">    }</span>

    public List&lt;org.apache.hadoop.mapreduce.InputSplit&gt; getSplits(JobContext context) throws IOException
    {
<span class="nc" id="L119">        Configuration conf = HadoopCompat.getConfiguration(context);</span>

<span class="nc" id="L121">        validateConfiguration(conf);</span>

<span class="nc" id="L123">        keyspace = ConfigHelper.getInputKeyspace(conf);</span>
<span class="nc" id="L124">        cfName = ConfigHelper.getInputColumnFamily(conf);</span>
<span class="nc" id="L125">        partitioner = ConfigHelper.getInputPartitioner(conf);</span>
<span class="nc" id="L126">        logger.trace(&quot;partitioner is {}&quot;, partitioner);</span>

        // canonical ranges, split into pieces, fetching the splits in parallel
<span class="nc" id="L129">        ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());</span>
<span class="nc" id="L130">        List&lt;org.apache.hadoop.mapreduce.InputSplit&gt; splits = new ArrayList&lt;&gt;();</span>

<span class="nc" id="L132">        try (Cluster cluster = CqlConfigHelper.getInputCluster(ConfigHelper.getInputInitialAddress(conf).split(&quot;,&quot;), conf);</span>
<span class="nc" id="L133">             Session session = cluster.connect())</span>
        {
<span class="nc" id="L135">            List&lt;Future&lt;List&lt;org.apache.hadoop.mapreduce.InputSplit&gt;&gt;&gt; splitfutures = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L136">            KeyRange jobKeyRange = ConfigHelper.getInputKeyRange(conf);</span>
<span class="nc" id="L137">            Range&lt;Token&gt; jobRange = null;</span>
<span class="nc bnc" id="L138" title="All 2 branches missed.">            if (jobKeyRange != null)</span>
            {
<span class="nc bnc" id="L140" title="All 2 branches missed.">                if (jobKeyRange.start_key != null)</span>
                {
<span class="nc bnc" id="L142" title="All 2 branches missed.">                    if (!partitioner.preservesOrder())</span>
<span class="nc" id="L143">                        throw new UnsupportedOperationException(&quot;KeyRange based on keys can only be used with a order preserving partitioner&quot;);</span>
<span class="nc bnc" id="L144" title="All 2 branches missed.">                    if (jobKeyRange.start_token != null)</span>
<span class="nc" id="L145">                        throw new IllegalArgumentException(&quot;only start_key supported&quot;);</span>
<span class="nc bnc" id="L146" title="All 2 branches missed.">                    if (jobKeyRange.end_token != null)</span>
<span class="nc" id="L147">                        throw new IllegalArgumentException(&quot;only start_key supported&quot;);</span>
<span class="nc" id="L148">                    jobRange = new Range&lt;&gt;(partitioner.getToken(jobKeyRange.start_key),</span>
<span class="nc" id="L149">                                           partitioner.getToken(jobKeyRange.end_key));</span>
                }
<span class="nc bnc" id="L151" title="All 2 branches missed.">                else if (jobKeyRange.start_token != null)</span>
                {
<span class="nc" id="L153">                    jobRange = new Range&lt;&gt;(partitioner.getTokenFactory().fromString(jobKeyRange.start_token),</span>
<span class="nc" id="L154">                                           partitioner.getTokenFactory().fromString(jobKeyRange.end_token));</span>
                }
                else
                {
<span class="nc" id="L158">                    logger.warn(&quot;ignoring jobKeyRange specified without start_key or start_token&quot;);</span>
                }
            }

<span class="nc" id="L162">            Metadata metadata = cluster.getMetadata();</span>

            // canonical ranges and nodes holding replicas
<span class="nc" id="L165">            Map&lt;TokenRange, Set&lt;Host&gt;&gt; masterRangeNodes = getRangeMap(keyspace, metadata);</span>

<span class="nc bnc" id="L167" title="All 2 branches missed.">            for (TokenRange range : masterRangeNodes.keySet())</span>
            {
<span class="nc bnc" id="L169" title="All 2 branches missed.">                if (jobRange == null)</span>
                {
                    // for each tokenRange, pick a live owner and ask it to compute bite-sized splits
<span class="nc" id="L172">                    splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf, session)));</span>
                }
                else
                {
<span class="nc" id="L176">                    TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange);</span>
<span class="nc bnc" id="L177" title="All 2 branches missed.">                    if (range.intersects(jobTokenRange))</span>
                    {
<span class="nc bnc" id="L179" title="All 2 branches missed.">                        for (TokenRange intersection: range.intersectWith(jobTokenRange))</span>
                        {
                            // for each tokenRange, pick a live owner and ask it to compute bite-sized splits
<span class="nc" id="L182">                            splitfutures.add(executor.submit(new SplitCallable(intersection,  masterRangeNodes.get(range), conf, session)));</span>
<span class="nc" id="L183">                        }</span>
                    }
                }
<span class="nc" id="L186">            }</span>

            // wait until we have all the results back
<span class="nc bnc" id="L189" title="All 2 branches missed.">            for (Future&lt;List&lt;org.apache.hadoop.mapreduce.InputSplit&gt;&gt; futureInputSplits : splitfutures)</span>
            {
                try
                {
<span class="nc" id="L193">                    splits.addAll(futureInputSplits.get());</span>
                }
<span class="nc" id="L195">                catch (Exception e)</span>
                {
<span class="nc" id="L197">                    throw new IOException(&quot;Could not get input splits&quot;, e);</span>
<span class="nc" id="L198">                }</span>
<span class="nc" id="L199">            }</span>
        }
        finally
        {
<span class="nc" id="L203">            executor.shutdownNow();</span>
        }

<span class="nc bnc" id="L206" title="All 4 branches missed.">        assert splits.size() &gt; 0;</span>
<span class="nc" id="L207">        Collections.shuffle(splits, new Random(System.nanoTime()));</span>
<span class="nc" id="L208">        return splits;</span>
    }

    private TokenRange rangeToTokenRange(Metadata metadata, Range&lt;Token&gt; range)
    {
<span class="nc" id="L213">        return metadata.newTokenRange(metadata.newToken(partitioner.getTokenFactory().toString(range.left)),</span>
<span class="nc" id="L214">                metadata.newToken(partitioner.getTokenFactory().toString(range.right)));</span>
    }

    private Map&lt;TokenRange, Long&gt; getSubSplits(String keyspace, String cfName, TokenRange range, Configuration conf, Session session)
    {
<span class="nc" id="L219">        int splitSize = ConfigHelper.getInputSplitSize(conf);</span>
<span class="nc" id="L220">        int splitSizeMb = ConfigHelper.getInputSplitSizeInMb(conf);</span>
        try
        {
<span class="nc" id="L223">            return describeSplits(keyspace, cfName, range, splitSize, splitSizeMb, session);</span>
        }
<span class="nc" id="L225">        catch (Exception e)</span>
        {
<span class="nc" id="L227">            throw new RuntimeException(e);</span>
        }
    }

    private Map&lt;TokenRange, Set&lt;Host&gt;&gt; getRangeMap(String keyspace, Metadata metadata)
    {
<span class="nc" id="L233">        return metadata.getTokenRanges()</span>
<span class="nc" id="L234">                       .stream()</span>
<span class="nc" id="L235">                       .collect(toMap(p -&gt; p, p -&gt; metadata.getReplicas('&quot;' + keyspace + '&quot;', p)));</span>
    }

    private Map&lt;TokenRange, Long&gt; describeSplits(String keyspace, String table, TokenRange tokenRange, int splitSize, int splitSizeMb, Session session)
    {
<span class="nc" id="L240">        String query = String.format(&quot;SELECT mean_partition_size, partitions_count &quot; +</span>
                                     &quot;FROM %s.%s &quot; +
                                     &quot;WHERE keyspace_name = ? AND table_name = ? AND range_start = ? AND range_end = ?&quot;,
                                     SchemaConstants.SYSTEM_KEYSPACE_NAME,
                                     SystemKeyspace.SIZE_ESTIMATES);

<span class="nc" id="L246">        ResultSet resultSet = session.execute(query, keyspace, table, tokenRange.getStart().toString(), tokenRange.getEnd().toString());</span>

<span class="nc" id="L248">        Row row = resultSet.one();</span>

<span class="nc" id="L250">        long meanPartitionSize = 0;</span>
<span class="nc" id="L251">        long partitionCount = 0;</span>
<span class="nc" id="L252">        int splitCount = 0;</span>

<span class="nc bnc" id="L254" title="All 2 branches missed.">        if (row != null)</span>
        {
<span class="nc" id="L256">            meanPartitionSize = row.getLong(&quot;mean_partition_size&quot;);</span>
<span class="nc" id="L257">            partitionCount = row.getLong(&quot;partitions_count&quot;);</span>

<span class="nc bnc" id="L259" title="All 2 branches missed.">            splitCount = splitSizeMb &gt; 0</span>
                ? (int)(meanPartitionSize * partitionCount / splitSizeMb / 1024 / 1024)
                : (int)(partitionCount / splitSize);
        }

        // If we have no data on this split or the size estimate is 0,
        // return the full split i.e., do not sub-split
        // Assume smallest granularity of partition count available from CASSANDRA-7688
<span class="nc bnc" id="L267" title="All 2 branches missed.">        if (splitCount == 0)</span>
        {
<span class="nc" id="L269">            Map&lt;TokenRange, Long&gt; wrappedTokenRange = new HashMap&lt;&gt;();</span>
<span class="nc" id="L270">            wrappedTokenRange.put(tokenRange, (long) 128);</span>
<span class="nc" id="L271">            return wrappedTokenRange;</span>
        }

<span class="nc" id="L274">        List&lt;TokenRange&gt; splitRanges = tokenRange.splitEvenly(splitCount);</span>
<span class="nc" id="L275">        Map&lt;TokenRange, Long&gt; rangesWithLength = new HashMap&lt;&gt;();</span>
<span class="nc bnc" id="L276" title="All 2 branches missed.">        for (TokenRange range : splitRanges)</span>
<span class="nc" id="L277">            rangesWithLength.put(range, partitionCount/splitCount);</span>

<span class="nc" id="L279">        return rangesWithLength;</span>
    }

    // Old Hadoop API
    public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException
    {
<span class="nc" id="L285">        TaskAttemptContext tac = HadoopCompat.newTaskAttemptContext(jobConf, new TaskAttemptID());</span>
<span class="nc" id="L286">        List&lt;org.apache.hadoop.mapreduce.InputSplit&gt; newInputSplits = this.getSplits(tac);</span>
<span class="nc" id="L287">        InputSplit[] oldInputSplits = new InputSplit[newInputSplits.size()];</span>
<span class="nc bnc" id="L288" title="All 2 branches missed.">        for (int i = 0; i &lt; newInputSplits.size(); i++)</span>
<span class="nc" id="L289">            oldInputSplits[i] = (ColumnFamilySplit)newInputSplits.get(i);</span>
<span class="nc" id="L290">        return oldInputSplits;</span>
    }

    /**
     * Gets a token tokenRange and splits it up according to the suggested
     * size into input splits that Hadoop can use.
     */
    class SplitCallable implements Callable&lt;List&lt;org.apache.hadoop.mapreduce.InputSplit&gt;&gt;
    {

        private final TokenRange tokenRange;
        private final Set&lt;Host&gt; hosts;
        private final Configuration conf;
        private final Session session;

        public SplitCallable(TokenRange tr, Set&lt;Host&gt; hosts, Configuration conf, Session session)
<span class="nc" id="L306">        {</span>
<span class="nc" id="L307">            this.tokenRange = tr;</span>
<span class="nc" id="L308">            this.hosts = hosts;</span>
<span class="nc" id="L309">            this.conf = conf;</span>
<span class="nc" id="L310">            this.session = session;</span>
<span class="nc" id="L311">        }</span>

        public List&lt;org.apache.hadoop.mapreduce.InputSplit&gt; call() throws Exception
        {
<span class="nc" id="L315">            ArrayList&lt;org.apache.hadoop.mapreduce.InputSplit&gt; splits = new ArrayList&lt;&gt;();</span>
            Map&lt;TokenRange, Long&gt; subSplits;
<span class="nc" id="L317">            subSplits = getSubSplits(keyspace, cfName, tokenRange, conf, session);</span>
            // turn the sub-ranges into InputSplits
<span class="nc" id="L319">            String[] endpoints = new String[hosts.size()];</span>

            // hadoop needs hostname, not ip
<span class="nc" id="L322">            int endpointIndex = 0;</span>
<span class="nc bnc" id="L323" title="All 2 branches missed.">            for (Host endpoint : hosts)</span>
<span class="nc" id="L324">                endpoints[endpointIndex++] = endpoint.getAddress().getHostName();</span>

<span class="nc bnc" id="L326" title="All 4 branches missed.">            boolean partitionerIsOpp = partitioner instanceof OrderPreservingPartitioner || partitioner instanceof ByteOrderedPartitioner;</span>

<span class="nc bnc" id="L328" title="All 2 branches missed.">            for (Map.Entry&lt;TokenRange, Long&gt; subSplitEntry : subSplits.entrySet())</span>
            {
<span class="nc" id="L330">                List&lt;TokenRange&gt; ranges = subSplitEntry.getKey().unwrap();</span>
<span class="nc bnc" id="L331" title="All 2 branches missed.">                for (TokenRange subrange : ranges)</span>
                {
<span class="nc bnc" id="L333" title="All 2 branches missed.">                    ColumnFamilySplit split =</span>
                            new ColumnFamilySplit(
                                    partitionerIsOpp ?
<span class="nc bnc" id="L336" title="All 2 branches missed.">                                            subrange.getStart().toString().substring(2) : subrange.getStart().toString(),</span>
                                    partitionerIsOpp ?
<span class="nc" id="L338">                                            subrange.getEnd().toString().substring(2) : subrange.getEnd().toString(),</span>
<span class="nc" id="L339">                                    subSplitEntry.getValue(),</span>
                                    endpoints);

<span class="nc" id="L342">                    logger.trace(&quot;adding {}&quot;, split);</span>
<span class="nc" id="L343">                    splits.add(split);</span>
<span class="nc" id="L344">                }</span>
<span class="nc" id="L345">            }</span>
<span class="nc" id="L346">            return splits;</span>
        }
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>