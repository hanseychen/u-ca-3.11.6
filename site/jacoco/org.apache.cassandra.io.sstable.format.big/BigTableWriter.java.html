<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>BigTableWriter.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">JaCoCo Ant Example</a> &gt; <a href="index.source.html" class="el_package">org.apache.cassandra.io.sstable.format.big</a> &gt; <span class="el_source">BigTableWriter.java</span></div><h1>BigTableWriter.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.io.sstable.format.big;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Collection;
import java.util.Map;
import java.util.Optional;

import org.apache.cassandra.db.lifecycle.LifecycleNewTracker;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.cassandra.cache.ChunkCache;
import org.apache.cassandra.config.CFMetaData;
import org.apache.cassandra.config.Config;
import org.apache.cassandra.config.DatabaseDescriptor;
import org.apache.cassandra.db.*;
import org.apache.cassandra.db.rows.*;
import org.apache.cassandra.db.transform.Transformation;
import org.apache.cassandra.io.FSWriteError;
import org.apache.cassandra.io.compress.CompressedSequentialWriter;
import org.apache.cassandra.io.sstable.*;
import org.apache.cassandra.io.sstable.format.SSTableFlushObserver;
import org.apache.cassandra.io.sstable.format.SSTableReader;
import org.apache.cassandra.io.sstable.format.SSTableWriter;
import org.apache.cassandra.io.sstable.metadata.MetadataCollector;
import org.apache.cassandra.io.sstable.metadata.MetadataComponent;
import org.apache.cassandra.io.sstable.metadata.MetadataType;
import org.apache.cassandra.io.sstable.metadata.StatsMetadata;
import org.apache.cassandra.io.util.*;
import org.apache.cassandra.utils.*;
import org.apache.cassandra.utils.concurrent.Transactional;

<span class="pc bpc" id="L53" title="1 of 2 branches missed.">public class BigTableWriter extends SSTableWriter</span>
{
<span class="fc" id="L55">    private static final Logger logger = LoggerFactory.getLogger(BigTableWriter.class);</span>

    private final ColumnIndex columnIndexWriter;
    private final IndexWriter iwriter;
    private final FileHandle.Builder dbuilder;
    protected final SequentialWriter dataFile;
    private DecoratedKey lastWrittenKey;
    private DataPosition dataMark;
<span class="fc" id="L63">    private long lastEarlyOpenLength = 0;</span>
<span class="fc" id="L64">    private final Optional&lt;ChunkCache&gt; chunkCache = Optional.ofNullable(ChunkCache.instance);</span>

<span class="fc" id="L66">    private final SequentialWriterOption writerOption = SequentialWriterOption.newBuilder()</span>
<span class="fc" id="L67">                                                        .trickleFsync(DatabaseDescriptor.getTrickleFsync())</span>
<span class="fc" id="L68">                                                        .trickleFsyncByteInterval(DatabaseDescriptor.getTrickleFsyncIntervalInKb() * 1024)</span>
<span class="fc" id="L69">                                                        .build();</span>

    public BigTableWriter(Descriptor descriptor,
                          long keyCount,
                          long repairedAt,
                          CFMetaData metadata,
                          MetadataCollector metadataCollector, 
                          SerializationHeader header,
                          Collection&lt;SSTableFlushObserver&gt; observers,
                          LifecycleNewTracker lifecycleNewTracker)
    {
<span class="fc" id="L80">        super(descriptor, keyCount, repairedAt, metadata, metadataCollector, header, observers);</span>
<span class="fc" id="L81">        lifecycleNewTracker.trackNew(this); // must track before any files are created</span>

<span class="pc bpc" id="L83" title="1 of 2 branches missed.">        if (compression)</span>
        {
<span class="fc" id="L85">            dataFile = new CompressedSequentialWriter(new File(getFilename()),</span>
<span class="fc" id="L86">                                             descriptor.filenameFor(Component.COMPRESSION_INFO),</span>
<span class="fc" id="L87">                                             new File(descriptor.filenameFor(descriptor.digestComponent)),</span>
                                             writerOption,
                                             metadata.params.compression,
                                             metadataCollector);
        }
        else
        {
<span class="nc" id="L94">            dataFile = new ChecksummedSequentialWriter(new File(getFilename()),</span>
<span class="nc" id="L95">                    new File(descriptor.filenameFor(Component.CRC)),</span>
<span class="nc" id="L96">                    new File(descriptor.filenameFor(descriptor.digestComponent)),</span>
                    writerOption);
        }
<span class="fc" id="L99">        dbuilder = new FileHandle.Builder(descriptor.filenameFor(Component.DATA)).compressed(compression)</span>
<span class="pc bpc" id="L100" title="1 of 2 branches missed.">                                              .mmapped(DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.mmap);</span>
<span class="fc" id="L101">        chunkCache.ifPresent(dbuilder::withChunkCache);</span>
<span class="fc" id="L102">        iwriter = new IndexWriter(keyCount);</span>

<span class="fc" id="L104">        columnIndexWriter = new ColumnIndex(this.header, dataFile, descriptor.version, this.observers, getRowIndexEntrySerializer().indexInfoSerializer());</span>
<span class="fc" id="L105">    }</span>

    public void mark()
    {
<span class="nc" id="L109">        dataMark = dataFile.mark();</span>
<span class="nc" id="L110">        iwriter.mark();</span>
<span class="nc" id="L111">    }</span>

    public void resetAndTruncate()
    {
<span class="nc" id="L115">        dataFile.resetAndTruncate(dataMark);</span>
<span class="nc" id="L116">        iwriter.resetAndTruncate();</span>
<span class="nc" id="L117">    }</span>

    /**
     * Perform sanity checks on @param decoratedKey and @return the position in the data file before any data is written
     */
    protected long beforeAppend(DecoratedKey decoratedKey)
    {
<span class="pc bpc" id="L124" title="2 of 4 branches missed.">        assert decoratedKey != null : &quot;Keys must not be null&quot;; // empty keys ARE allowed b/c of indexed column values</span>
<span class="pc bpc" id="L125" title="1 of 4 branches missed.">        if (lastWrittenKey != null &amp;&amp; lastWrittenKey.compareTo(decoratedKey) &gt;= 0)</span>
<span class="nc" id="L126">            throw new RuntimeException(&quot;Last written key &quot; + lastWrittenKey + &quot; &gt;= current key &quot; + decoratedKey + &quot; writing into &quot; + getFilename());</span>
<span class="fc bfc" id="L127" title="All 2 branches covered.">        return (lastWrittenKey == null) ? 0 : dataFile.position();</span>
    }

    private void afterAppend(DecoratedKey decoratedKey, long dataEnd, RowIndexEntry index, ByteBuffer indexInfo) throws IOException
    {
<span class="fc" id="L132">        metadataCollector.addKey(decoratedKey.getKey());</span>
<span class="fc" id="L133">        lastWrittenKey = decoratedKey;</span>
<span class="fc" id="L134">        last = lastWrittenKey;</span>
<span class="fc bfc" id="L135" title="All 2 branches covered.">        if (first == null)</span>
<span class="fc" id="L136">            first = lastWrittenKey;</span>

<span class="pc bpc" id="L138" title="1 of 2 branches missed.">        if (logger.isTraceEnabled())</span>
<span class="nc" id="L139">            logger.trace(&quot;wrote {} at {}&quot;, decoratedKey, dataEnd);</span>
<span class="fc" id="L140">        iwriter.append(decoratedKey, index, dataEnd, indexInfo);</span>
<span class="fc" id="L141">    }</span>

    /**
     * Appends partition data to this writer.
     *
     * @param iterator the partition to write
     * @return the created index entry if something was written, that is if {@code iterator}
     * wasn't empty, {@code null} otherwise.
     *
     * @throws FSWriteError if a write to the dataFile fails
     */
    public RowIndexEntry append(UnfilteredRowIterator iterator)
    {
<span class="fc" id="L154">        DecoratedKey key = iterator.partitionKey();</span>

<span class="pc bpc" id="L156" title="1 of 2 branches missed.">        if (key.getKey().remaining() &gt; FBUtilities.MAX_UNSIGNED_SHORT)</span>
        {
<span class="nc" id="L158">            logger.error(&quot;Key size {} exceeds maximum of {}, skipping row&quot;, key.getKey().remaining(), FBUtilities.MAX_UNSIGNED_SHORT);</span>
<span class="nc" id="L159">            return null;</span>
        }

<span class="pc bpc" id="L162" title="1 of 2 branches missed.">        if (iterator.isEmpty())</span>
<span class="nc" id="L163">            return null;</span>

<span class="fc" id="L165">        long startPosition = beforeAppend(key);</span>
<span class="pc" id="L166">        observers.forEach((o) -&gt; o.startPartition(key, iwriter.indexFile.position()));</span>

        //Reuse the writer for each row
<span class="fc" id="L169">        columnIndexWriter.reset();</span>

<span class="fc" id="L171">        try (UnfilteredRowIterator collecting = Transformation.apply(iterator, new StatsCollector(metadataCollector)))</span>
        {
<span class="fc" id="L173">            columnIndexWriter.buildRowIndex(collecting);</span>

            // afterAppend() writes the partition key before the first RowIndexEntry - so we have to add it's
            // serialized size to the index-writer position
<span class="fc" id="L177">            long indexFilePosition = ByteBufferUtil.serializedSizeWithShortLength(key.getKey()) + iwriter.indexFile.position();</span>

<span class="fc" id="L179">            RowIndexEntry entry = RowIndexEntry.create(startPosition, indexFilePosition,</span>
<span class="fc" id="L180">                                                       collecting.partitionLevelDeletion(),</span>
                                                       columnIndexWriter.headerLength,
                                                       columnIndexWriter.columnIndexCount,
<span class="fc" id="L183">                                                       columnIndexWriter.indexInfoSerializedSize(),</span>
<span class="fc" id="L184">                                                       columnIndexWriter.indexSamples(),</span>
<span class="fc" id="L185">                                                       columnIndexWriter.offsets(),</span>
<span class="fc" id="L186">                                                       getRowIndexEntrySerializer().indexInfoSerializer());</span>

<span class="fc" id="L188">            long endPosition = dataFile.position();</span>
<span class="fc" id="L189">            long rowSize = endPosition - startPosition;</span>
<span class="fc" id="L190">            maybeLogLargePartitionWarning(key, rowSize);</span>
<span class="fc" id="L191">            metadataCollector.addPartitionSizeInBytes(rowSize);</span>
<span class="fc" id="L192">            afterAppend(key, endPosition, entry, columnIndexWriter.buffer());</span>
<span class="fc" id="L193">            return entry;</span>
        }
<span class="nc" id="L195">        catch (IOException e)</span>
        {
<span class="nc" id="L197">            throw new FSWriteError(e, dataFile.getPath());</span>
        }
    }

    private RowIndexEntry.IndexSerializer&lt;IndexInfo&gt; getRowIndexEntrySerializer()
    {
<span class="fc" id="L203">        return (RowIndexEntry.IndexSerializer&lt;IndexInfo&gt;) rowIndexEntrySerializer;</span>
    }

    private void maybeLogLargePartitionWarning(DecoratedKey key, long rowSize)
    {
<span class="pc bpc" id="L208" title="1 of 2 branches missed.">        if (rowSize &gt; DatabaseDescriptor.getCompactionLargePartitionWarningThreshold())</span>
        {
<span class="nc" id="L210">            String keyString = metadata.getKeyValidator().getString(key.getKey());</span>
<span class="nc" id="L211">            logger.warn(&quot;Writing large partition {}/{}:{} ({}) to sstable {}&quot;, metadata.ksName, metadata.cfName, keyString, FBUtilities.prettyPrintMemory(rowSize), getFilename());</span>
        }
<span class="fc" id="L213">    }</span>

    private static class StatsCollector extends Transformation
    {
        private final MetadataCollector collector;
        private int cellCount;

        StatsCollector(MetadataCollector collector)
<span class="fc" id="L221">        {</span>
<span class="fc" id="L222">            this.collector = collector;</span>
<span class="fc" id="L223">        }</span>

        @Override
        public Row applyToStatic(Row row)
        {
<span class="pc bpc" id="L228" title="1 of 2 branches missed.">            if (!row.isEmpty())</span>
<span class="nc" id="L229">                cellCount += Rows.collectStats(row, collector);</span>
<span class="fc" id="L230">            return row;</span>
        }

        @Override
        public Row applyToRow(Row row)
        {
<span class="fc" id="L236">            collector.updateClusteringValues(row.clustering());</span>
<span class="fc" id="L237">            cellCount += Rows.collectStats(row, collector);</span>
<span class="fc" id="L238">            return row;</span>
        }

        @Override
        public RangeTombstoneMarker applyToMarker(RangeTombstoneMarker marker)
        {
<span class="nc" id="L244">            collector.updateClusteringValues(marker.clustering());</span>
<span class="nc bnc" id="L245" title="All 2 branches missed.">            if (marker.isBoundary())</span>
            {
<span class="nc" id="L247">                RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;</span>
<span class="nc" id="L248">                collector.update(bm.endDeletionTime());</span>
<span class="nc" id="L249">                collector.update(bm.startDeletionTime());</span>
<span class="nc" id="L250">            }</span>
            else
            {
<span class="nc" id="L253">                collector.update(((RangeTombstoneBoundMarker)marker).deletionTime());</span>
            }
<span class="nc" id="L255">            return marker;</span>
        }

        @Override
        public void onPartitionClose()
        {
<span class="fc" id="L261">            collector.addCellPerPartitionCount(cellCount);</span>
<span class="fc" id="L262">        }</span>

        @Override
        public DeletionTime applyToDeletion(DeletionTime deletionTime)
        {
<span class="fc" id="L267">            collector.update(deletionTime);</span>
<span class="fc" id="L268">            return deletionTime;</span>
        }
    }

    @SuppressWarnings(&quot;resource&quot;)
    public SSTableReader openEarly()
    {
        // find the max (exclusive) readable key
<span class="nc" id="L276">        IndexSummaryBuilder.ReadableBoundary boundary = iwriter.getMaxReadable();</span>
<span class="nc bnc" id="L277" title="All 2 branches missed.">        if (boundary == null)</span>
<span class="nc" id="L278">            return null;</span>

<span class="nc" id="L280">        StatsMetadata stats = statsMetadata();</span>
<span class="nc bnc" id="L281" title="All 6 branches missed.">        assert boundary.indexLength &gt; 0 &amp;&amp; boundary.dataLength &gt; 0;</span>
        // open the reader early
<span class="nc" id="L283">        IndexSummary indexSummary = iwriter.summary.build(metadata.partitioner, boundary);</span>
<span class="nc" id="L284">        long indexFileLength = new File(descriptor.filenameFor(Component.PRIMARY_INDEX)).length();</span>
<span class="nc" id="L285">        int indexBufferSize = optimizationStrategy.bufferSize(indexFileLength / indexSummary.size());</span>
<span class="nc" id="L286">        FileHandle ifile = iwriter.builder.bufferSize(indexBufferSize).complete(boundary.indexLength);</span>
<span class="nc bnc" id="L287" title="All 2 branches missed.">        if (compression)</span>
<span class="nc" id="L288">            dbuilder.withCompressionMetadata(((CompressedSequentialWriter) dataFile).open(boundary.dataLength));</span>
<span class="nc" id="L289">        int dataBufferSize = optimizationStrategy.bufferSize(stats.estimatedPartitionSize.percentile(DatabaseDescriptor.getDiskOptimizationEstimatePercentile()));</span>
<span class="nc" id="L290">        FileHandle dfile = dbuilder.bufferSize(dataBufferSize).complete(boundary.dataLength);</span>
<span class="nc" id="L291">        invalidateCacheAtBoundary(dfile);</span>
<span class="nc" id="L292">        SSTableReader sstable = SSTableReader.internalOpen(descriptor,</span>
                                                           components, metadata,
                                                           ifile, dfile, indexSummary,
<span class="nc" id="L295">                                                           iwriter.bf.sharedCopy(), maxDataAge, stats, SSTableReader.OpenReason.EARLY, header);</span>

        // now it's open, find the ACTUAL last readable key (i.e. for which the data file has also been flushed)
<span class="nc" id="L298">        sstable.first = getMinimalKey(first);</span>
<span class="nc" id="L299">        sstable.last = getMinimalKey(boundary.lastKey);</span>
<span class="nc" id="L300">        return sstable;</span>
    }

    void invalidateCacheAtBoundary(FileHandle dfile)
    {
<span class="fc" id="L305">        chunkCache.ifPresent(cache -&gt; {</span>
<span class="pc bpc" id="L306" title="1 of 4 branches missed.">            if (lastEarlyOpenLength != 0 &amp;&amp; dfile.dataLength() &gt; lastEarlyOpenLength)</span>
<span class="nc" id="L307">                cache.invalidatePosition(dfile, lastEarlyOpenLength);</span>
<span class="fc" id="L308">        });</span>
<span class="fc" id="L309">        lastEarlyOpenLength = dfile.dataLength();</span>
<span class="fc" id="L310">    }</span>

    public SSTableReader openFinalEarly()
    {
        // we must ensure the data is completely flushed to disk
<span class="fc" id="L315">        dataFile.sync();</span>
<span class="fc" id="L316">        iwriter.indexFile.sync();</span>

<span class="fc" id="L318">        return openFinal(SSTableReader.OpenReason.EARLY);</span>
    }

    @SuppressWarnings(&quot;resource&quot;)
    private SSTableReader openFinal(SSTableReader.OpenReason openReason)
    {
<span class="fc bfc" id="L324" title="All 2 branches covered.">        if (maxDataAge &lt; 0)</span>
<span class="fc" id="L325">            maxDataAge = System.currentTimeMillis();</span>

<span class="fc" id="L327">        StatsMetadata stats = statsMetadata();</span>
        // finalize in-memory state for the reader
<span class="fc" id="L329">        IndexSummary indexSummary = iwriter.summary.build(this.metadata.partitioner);</span>
<span class="fc" id="L330">        long indexFileLength = new File(descriptor.filenameFor(Component.PRIMARY_INDEX)).length();</span>
<span class="fc" id="L331">        int dataBufferSize = optimizationStrategy.bufferSize(stats.estimatedPartitionSize.percentile(DatabaseDescriptor.getDiskOptimizationEstimatePercentile()));</span>
<span class="fc" id="L332">        int indexBufferSize = optimizationStrategy.bufferSize(indexFileLength / indexSummary.size());</span>
<span class="fc" id="L333">        FileHandle ifile = iwriter.builder.bufferSize(indexBufferSize).complete();</span>
<span class="pc bpc" id="L334" title="1 of 2 branches missed.">        if (compression)</span>
<span class="fc" id="L335">            dbuilder.withCompressionMetadata(((CompressedSequentialWriter) dataFile).open(0));</span>
<span class="fc" id="L336">        FileHandle dfile = dbuilder.bufferSize(dataBufferSize).complete();</span>
<span class="fc" id="L337">        invalidateCacheAtBoundary(dfile);</span>
<span class="fc" id="L338">        SSTableReader sstable = SSTableReader.internalOpen(descriptor,</span>
                                                           components,
                                                           this.metadata,
                                                           ifile,
                                                           dfile,
                                                           indexSummary,
<span class="fc" id="L344">                                                           iwriter.bf.sharedCopy(),</span>
                                                           maxDataAge,
                                                           stats,
                                                           openReason,
                                                           header);
<span class="fc" id="L349">        sstable.first = getMinimalKey(first);</span>
<span class="fc" id="L350">        sstable.last = getMinimalKey(last);</span>
<span class="fc" id="L351">        return sstable;</span>
    }

    protected SSTableWriter.TransactionalProxy txnProxy()
    {
<span class="fc" id="L356">        return new TransactionalProxy();</span>
    }

<span class="fc" id="L359">    class TransactionalProxy extends SSTableWriter.TransactionalProxy</span>
    {
        // finalise our state on disk, including renaming
        protected void doPrepare()
        {
<span class="fc" id="L364">            iwriter.prepareToCommit();</span>

            // write sstable statistics
<span class="fc" id="L367">            dataFile.prepareToCommit();</span>
<span class="fc" id="L368">            writeMetadata(descriptor, finalizeMetadata());</span>

            // save the table of components
<span class="fc" id="L371">            SSTable.appendTOC(descriptor, components);</span>

<span class="pc bpc" id="L373" title="1 of 2 branches missed.">            if (openResult)</span>
<span class="fc" id="L374">                finalReader = openFinal(SSTableReader.OpenReason.NORMAL);</span>
<span class="fc" id="L375">        }</span>

        protected Throwable doCommit(Throwable accumulate)
        {
<span class="fc" id="L379">            accumulate = dataFile.commit(accumulate);</span>
<span class="fc" id="L380">            accumulate = iwriter.commit(accumulate);</span>
<span class="fc" id="L381">            return accumulate;</span>
        }

        @Override
        protected Throwable doPostCleanup(Throwable accumulate)
        {
<span class="fc" id="L387">            accumulate = dbuilder.close(accumulate);</span>
<span class="fc" id="L388">            return accumulate;</span>
        }

        protected Throwable doAbort(Throwable accumulate)
        {
<span class="nc" id="L393">            accumulate = iwriter.abort(accumulate);</span>
<span class="nc" id="L394">            accumulate = dataFile.abort(accumulate);</span>
<span class="nc" id="L395">            return accumulate;</span>
        }
    }

    private void writeMetadata(Descriptor desc, Map&lt;MetadataType, MetadataComponent&gt; components)
    {
<span class="fc" id="L401">        File file = new File(desc.filenameFor(Component.STATS));</span>
<span class="fc" id="L402">        try (SequentialWriter out = new SequentialWriter(file, writerOption))</span>
        {
<span class="fc" id="L404">            desc.getMetadataSerializer().serialize(components, out, desc.version);</span>
<span class="fc" id="L405">            out.finish();</span>
        }
<span class="nc" id="L407">        catch (IOException e)</span>
        {
<span class="nc" id="L409">            throw new FSWriteError(e, file.getPath());</span>
<span class="fc" id="L410">        }</span>
<span class="fc" id="L411">    }</span>

    public long getFilePointer()
    {
<span class="fc" id="L415">        return dataFile.position();</span>
    }

    public long getOnDiskFilePointer()
    {
<span class="nc" id="L420">        return dataFile.getOnDiskFilePointer();</span>
    }

    public long getEstimatedOnDiskBytesWritten()
    {
<span class="nc" id="L425">        return dataFile.getEstimatedOnDiskBytesWritten();</span>
    }

    /**
     * Encapsulates writing the index and filter for an SSTable. The state of this object is not valid until it has been closed.
     */
    class IndexWriter extends AbstractTransactional implements Transactional
    {
        private final SequentialWriter indexFile;
        public final FileHandle.Builder builder;
        public final IndexSummaryBuilder summary;
        public final IFilter bf;
        private DataPosition mark;

        IndexWriter(long keyCount)
<span class="fc" id="L440">        {</span>
<span class="fc" id="L441">            indexFile = new SequentialWriter(new File(descriptor.filenameFor(Component.PRIMARY_INDEX)), writerOption);</span>
<span class="pc bpc" id="L442" title="1 of 2 branches missed.">            builder = new FileHandle.Builder(descriptor.filenameFor(Component.PRIMARY_INDEX)).mmapped(DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.mmap);</span>
<span class="fc" id="L443">            chunkCache.ifPresent(builder::withChunkCache);</span>
<span class="fc" id="L444">            summary = new IndexSummaryBuilder(keyCount, metadata.params.minIndexInterval, Downsampling.BASE_SAMPLING_LEVEL);</span>
<span class="fc" id="L445">            bf = FilterFactory.getFilter(keyCount, metadata.params.bloomFilterFpChance, true, descriptor.version.hasOldBfHashOrder());</span>
            // register listeners to be alerted when the data files are flushed
<span class="fc" id="L447">            indexFile.setPostFlushListener(() -&gt; summary.markIndexSynced(indexFile.getLastFlushOffset()));</span>
<span class="fc" id="L448">            dataFile.setPostFlushListener(() -&gt; summary.markDataSynced(dataFile.getLastFlushOffset()));</span>
<span class="fc" id="L449">        }</span>

        // finds the last (-offset) decorated key that can be guaranteed to occur fully in the flushed portion of the index file
        IndexSummaryBuilder.ReadableBoundary getMaxReadable()
        {
<span class="nc" id="L454">            return summary.getLastReadableBoundary();</span>
        }

        public void append(DecoratedKey key, RowIndexEntry indexEntry, long dataEnd, ByteBuffer indexInfo) throws IOException
        {
<span class="fc" id="L459">            bf.add(key);</span>
<span class="fc" id="L460">            long indexStart = indexFile.position();</span>
            try
            {
<span class="fc" id="L463">                ByteBufferUtil.writeWithShortLength(key.getKey(), indexFile);</span>
<span class="fc" id="L464">                rowIndexEntrySerializer.serialize(indexEntry, indexFile, indexInfo);</span>
            }
<span class="nc" id="L466">            catch (IOException e)</span>
            {
<span class="nc" id="L468">                throw new FSWriteError(e, indexFile.getPath());</span>
<span class="fc" id="L469">            }</span>
<span class="fc" id="L470">            long indexEnd = indexFile.position();</span>

<span class="pc bpc" id="L472" title="1 of 2 branches missed.">            if (logger.isTraceEnabled())</span>
<span class="nc" id="L473">                logger.trace(&quot;wrote index entry: {} at {}&quot;, indexEntry, indexStart);</span>

<span class="fc" id="L475">            summary.maybeAddEntry(key, indexStart, indexEnd, dataEnd);</span>
<span class="fc" id="L476">        }</span>

        /**
         * Closes the index and bloomfilter, making the public state of this writer valid for consumption.
         */
        void flushBf()
        {
<span class="pc bpc" id="L483" title="1 of 2 branches missed.">            if (components.contains(Component.FILTER))</span>
            {
<span class="fc" id="L485">                String path = descriptor.filenameFor(Component.FILTER);</span>
<span class="fc" id="L486">                try (FileOutputStream fos = new FileOutputStream(path);</span>
<span class="fc" id="L487">                     DataOutputStreamPlus stream = new BufferedDataOutputStreamPlus(fos))</span>
                {
                    // bloom filter
<span class="fc" id="L490">                    FilterFactory.serialize(bf, stream);</span>
<span class="fc" id="L491">                    stream.flush();</span>
<span class="fc" id="L492">                    SyncUtil.sync(fos);</span>
                }
<span class="nc" id="L494">                catch (IOException e)</span>
                {
<span class="nc" id="L496">                    throw new FSWriteError(e, path);</span>
<span class="fc" id="L497">                }</span>
            }
<span class="fc" id="L499">        }</span>

        public void mark()
        {
<span class="nc" id="L503">            mark = indexFile.mark();</span>
<span class="nc" id="L504">        }</span>

        public void resetAndTruncate()
        {
            // we can't un-set the bloom filter addition, but extra keys in there are harmless.
            // we can't reset dbuilder either, but that is the last thing called in afterappend so
            // we assume that if that worked then we won't be trying to reset.
<span class="nc" id="L511">            indexFile.resetAndTruncate(mark);</span>
<span class="nc" id="L512">        }</span>

        protected void doPrepare()
        {
<span class="fc" id="L516">            flushBf();</span>

            // truncate index file
<span class="fc" id="L519">            long position = indexFile.position();</span>
<span class="fc" id="L520">            indexFile.prepareToCommit();</span>
<span class="fc" id="L521">            FileUtils.truncate(indexFile.getPath(), position);</span>

            // save summary
<span class="fc" id="L524">            summary.prepareToCommit();</span>
<span class="fc" id="L525">            try (IndexSummary indexSummary = summary.build(getPartitioner()))</span>
            {
<span class="fc" id="L527">                SSTableReader.saveSummary(descriptor, first, last, indexSummary);</span>
            }
<span class="fc" id="L529">        }</span>

        protected Throwable doCommit(Throwable accumulate)
        {
<span class="fc" id="L533">            return indexFile.commit(accumulate);</span>
        }

        protected Throwable doAbort(Throwable accumulate)
        {
<span class="nc" id="L538">            return indexFile.abort(accumulate);</span>
        }

        @Override
        protected Throwable doPostCleanup(Throwable accumulate)
        {
<span class="fc" id="L544">            accumulate = summary.close(accumulate);</span>
<span class="fc" id="L545">            accumulate = bf.close(accumulate);</span>
<span class="fc" id="L546">            accumulate = builder.close(accumulate);</span>
<span class="fc" id="L547">            return accumulate;</span>
        }
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>